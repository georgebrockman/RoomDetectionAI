{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Using inbuilt loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_dir = '/Users/georgebrockman/code/georgebrockman/Autoenhance.ai/RoomDetection/images/training_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "IMG_WIDTH, IMG_HEIGHT = 224, 224\n",
    "IMG_SIZE = IMG_WIDTH, IMG_HEIGHT\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27666 files belonging to 9 classes.\n",
      "Using 22133 files for training.\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27666 files belonging to 9 classes.\n",
      "Using 5533 files for validation.\n"
     ]
    }
   ],
   "source": [
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Import the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# instantiate the MobileNet V2\n",
    "IMG_SHAPE = IMG_SIZE + (3,)\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                             include_top=False,\n",
    "                                             weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 7, 7, 1280)\n"
     ]
    }
   ],
   "source": [
    "image_batch, label_batch = next(iter(train_ds)) # the next iteration in the dataset, so the first image\n",
    "feature_batch = base_model(image_batch)\n",
    "print(feature_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# freeze the convolutional base\n",
    "base_model.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 1280)\n"
     ]
    }
   ],
   "source": [
    "# convert the features to a single 1280-element vector per image\n",
    "global_av_layer = tf.keras.layers.GlobalAveragePooling2D() # averages over a 5x5 spatial \n",
    "feature_batch_av = global_av_layer(feature_batch)\n",
    "print(feature_batch_av.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply a dense layer to convert these features into a single prediction per image\n",
    "# no activation needed as the prediction will be treated as a logit (mapping of probabilities to Real Numbers)\n",
    "\n",
    "pred_layer = tf.keras.layers.Dense(1)\n",
    "pred_batch = pred_layer(feature_batch_av)\n",
    "pred_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# augmentate the data \n",
    "\n",
    "data_aug = tf.keras.Sequential([\n",
    "    tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomZoom(.5, .2),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomContrast((0.5,0.5), seed=1),\n",
    "])\n",
    "\n",
    "# rescale the pixel values to match the expected values of the MobileNetV2 model\n",
    "\n",
    "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# chain together data augmentation, rescaling, base_model and feature extractor layers useing the Keras Functional API\n",
    "\n",
    "inputs = tf.keras.Input(shape=(224,224,3)) # image size and channels\n",
    "# data augmentation layer\n",
    "x = data_aug(inputs)\n",
    "# preprocess, feed x into and reassign variable\n",
    "x = preprocess_input(x)\n",
    "# basemodel, set training =False for the BN layer\n",
    "x = base_model(x, training=False)\n",
    "# feature extraction\n",
    "x = global_av_layer(x)\n",
    "# add a dropout layer \n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "\n",
    "outputs = pred_layer(x)\n",
    "model = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "      tf.keras.metrics.TruePositives(name='tp'),\n",
    "      tf.keras.metrics.FalsePositives(name='fp'),\n",
    "      tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "      tf.keras.metrics.FalseNegatives(name='fn'), \n",
    "      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      tf.keras.metrics.Precision(name='precision'),\n",
    "      tf.keras.metrics.Recall(name='recall'),\n",
    "      tf.keras.metrics.AUC(name='auc'),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "base_learning_rate = 0.0001\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\n",
    "              # Only two linear outputs so use BinaryCrossentropy and logits =True\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(outputs),\n",
    "                                                 outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "692/692 [==============================] - 587s 849ms/step - loss: -131.0499 - accuracy: 0.2013 - val_loss: -153.7231 - val_accuracy: 0.1984\n",
      "Epoch 2/10\n",
      "692/692 [==============================] - 515s 744ms/step - loss: -235.4192 - accuracy: 0.2013 - val_loss: -240.4066 - val_accuracy: 0.1984\n",
      "Epoch 3/10\n",
      "692/692 [==============================] - 516s 746ms/step - loss: -339.4034 - accuracy: 0.2013 - val_loss: -326.8677 - val_accuracy: 0.1984\n",
      "Epoch 4/10\n",
      "692/692 [==============================] - 520s 751ms/step - loss: -441.2041 - accuracy: 0.2013 - val_loss: -413.2120 - val_accuracy: 0.1984\n",
      "Epoch 5/10\n",
      "692/692 [==============================] - 531s 768ms/step - loss: -545.3388 - accuracy: 0.2013 - val_loss: -499.2839 - val_accuracy: 0.1984\n",
      "Epoch 6/10\n",
      "692/692 [==============================] - 516s 746ms/step - loss: -650.7616 - accuracy: 0.2013 - val_loss: -585.6879 - val_accuracy: 0.1984\n",
      "Epoch 7/10\n",
      "692/692 [==============================] - 538s 778ms/step - loss: -755.1579 - accuracy: 0.2013 - val_loss: -672.1534 - val_accuracy: 0.1984\n",
      "Epoch 8/10\n",
      "692/692 [==============================] - 519s 750ms/step - loss: -859.0800 - accuracy: 0.2013 - val_loss: -758.6680 - val_accuracy: 0.1984\n",
      "Epoch 9/10\n",
      "692/692 [==============================] - 547s 791ms/step - loss: -963.1895 - accuracy: 0.2013 - val_loss: -845.2006 - val_accuracy: 0.1984\n",
      "Epoch 10/10\n",
      "692/692 [==============================] - 517s 748ms/step - loss: -1063.6646 - accuracy: 0.2013 - val_loss: -931.3663 - val_accuracy: 0.1984\n"
     ]
    }
   ],
   "source": [
    "initial_epochs = 10\n",
    "history = model.fit(train_ds,\n",
    "                    epochs=initial_epochs,\n",
    "                    validation_data=val_ds,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# balance the classes to help with the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using custom built loading function to build the model (with no valiadtion set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_classifcation(path, resize_h, resize_w, train=True, limit=None):\n",
    "    \n",
    "    # list all paths to data classes except DS_Store\n",
    "    class_folders = [f for f in sorted(os.listdir(path)) if not f.startswith('.')]\n",
    "    # load images\n",
    "    images = []\n",
    "    classes = []\n",
    "    for i, c in enumerate(class_folders):\n",
    "        #images_per_class = sorted(os.path.join(path, c))\n",
    "        images_per_class = [f for f in sorted(os.listdir(os.path.join(path, c))) if 'jpg' in f]\n",
    "        image_class = np.zeros(len(class_folders))\n",
    "        image_class[i] = 1\n",
    "        \n",
    "        for image_per_class in images_per_class:\n",
    "            images.append(os.path.join(path, c, image_per_class))\n",
    "            # the index will be the class label\n",
    "            classes.append(image_class)\n",
    "    \n",
    "    random.seed(10)\n",
    "    images_shuffle = random.sample(images, len(images))\n",
    "    classes_shuffle = random.sample(classes, len(classes))\n",
    "    train_test_split = 0.1\n",
    "    number_of_test = int(len(images) * train_test_split)\n",
    "    if train == False:\n",
    "        images = images_shuffle[0:number_of_test]\n",
    "        classes = classes_shuffle[0:number_of_test]\n",
    "    else:\n",
    "        images = images_shuffle[number_of_test:len(images)]\n",
    "        classes = classes_shuffle[number_of_test:len(images)]\n",
    "    \n",
    "    images_tf = tf.data.Dataset.from_tensor_slices(images)\n",
    "    classes_tf = tf.data.Dataset.from_tensor_slices(classes)\n",
    "    # put two arrays together so that each image has its classifying label \n",
    "    dataset = tf.data.Dataset.zip((images_tf, classes_tf))\n",
    "    \n",
    "    @tf.function\n",
    "    def read_images(image_path, class_type, mirrored=False):\n",
    "        image = tf.io.read_file(image_path)\n",
    "        image = tf.image.decode_jpeg(image)\n",
    "\n",
    "        h, w, c = image.shape\n",
    "        if not (h == resize_h and w == resize_w):\n",
    "            image = tf.image.resize(\n",
    "            image, [resize_h, resize_w], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "            # set all images shape to RGB\n",
    "            image.set_shape((224, 224, 3))\n",
    "#             print(image.shape)\n",
    "    \n",
    "    \n",
    "        # change DType of image to float32\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        class_type = tf.cast(class_type, tf.float32)\n",
    "        \n",
    "        # normalise the image pixels\n",
    "        image = (image / 255.0)\n",
    "\n",
    "        return image, class_type\n",
    "\n",
    "    dataset = dataset.map(\n",
    "        read_images,\n",
    "        num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "        deterministic=False)\n",
    "\n",
    "    return dataset, len(class_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data \n",
    "\n",
    "path = '/Users/georgebrockman/code/georgebrockman/Autoenhance.ai/RoomDetection/images/training_data/'\n",
    "train_dataset, num_classes = dataset_classifcation(path, 224, 224)\n",
    "test_dataset, num_classes = dataset_classifcation(path, 224, 224, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ParallelMapDataset shapes: ((224, 224, 3), (9,)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH, IMG_HEIGHT = 224, 224\n",
    "IMG_SIZE = IMG_WIDTH, IMG_HEIGHT\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "train_dataset = train_dataset.cache().shuffle(1000).batch(32).prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = test_dataset.cache().batch(32).prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the base model\n",
    "# instantiate the MobileNet V2\n",
    "IMG_SHAPE = IMG_SIZE + (3,)\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                             include_top=False,\n",
    "                                             weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 7, 7, 1280)\n"
     ]
    }
   ],
   "source": [
    "image_batch, label_batch = next(iter(train_dataset))# the next iteration in the dataset, so the first image\n",
    "feature_batch = base_model(image_batch)\n",
    "print(feature_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze the convolutional base\n",
    "base_model.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 1280)\n"
     ]
    }
   ],
   "source": [
    "# convert the features to a single 1280-element vector per image\n",
    "global_av_layer = tf.keras.layers.GlobalAveragePooling2D() # averages over a 5x5 spatial \n",
    "feature_batch_av = global_av_layer(feature_batch)\n",
    "print(feature_batch_av.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 9])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply a dense layer to convert these features into a single prediction per image\n",
    "# no activation needed as the prediction will be treated as a logit (mapping of probabilities to Real Numbers)\n",
    "\n",
    "pred_layer = tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "pred_batch = pred_layer(feature_batch_av)\n",
    "pred_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model volume 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmentate the data \n",
    "\n",
    "data_aug = tf.keras.Sequential([\n",
    "    tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomRotation(0.25),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomZoom(.5, .2),\n",
    "    \n",
    "])\n",
    "\n",
    "# rescale the pixel values to match the expected values of the MobileNetV2 model\n",
    "\n",
    "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain together data augmentation, rescaling, base_model and feature extractor layers useing the Keras Functional API\n",
    "\n",
    "inputs = tf.keras.Input(shape=(224,224,3)) # image size and channels\n",
    "# data augmentation layer\n",
    "x = data_aug(inputs)\n",
    "# preprocess, feed x into and reassign variable\n",
    "x = preprocess_input(x)\n",
    "# basemodel, set training =False for the BN layer\n",
    "x = base_model(x, training=False)\n",
    "# feature extraction\n",
    "x = global_av_layer(x)\n",
    "# add a dropout layer \n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "\n",
    "outputs = pred_layer(x)\n",
    "model = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_learning_rate = 0.0001\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\n",
    "              # Only two linear outputs so use BinaryCrossentropy and logits =True\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"/Users/georgebrockman/code/georgebrockman/Autoenhance.ai/RoomDetection/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "692/692 [==============================] - ETA: 0s - loss: 2.1399 - accuracy: 0.1903\n",
      "Epoch 00001: saving model to /Users/georgebrockman/code/georgebrockman/Autoenhance.ai/RoomDetection/cp.ckpt\n",
      "692/692 [==============================] - 488s 705ms/step - loss: 2.1399 - accuracy: 0.1903 - val_loss: 2.1346 - val_accuracy: 0.2087\n",
      "Epoch 2/10\n",
      "692/692 [==============================] - ETA: 0s - loss: 2.1392 - accuracy: 0.1959\n",
      "Epoch 00002: saving model to /Users/georgebrockman/code/georgebrockman/Autoenhance.ai/RoomDetection/cp.ckpt\n",
      "692/692 [==============================] - 482s 696ms/step - loss: 2.1392 - accuracy: 0.1959 - val_loss: 2.1348 - val_accuracy: 0.2087\n",
      "Epoch 3/10\n",
      "692/692 [==============================] - ETA: 0s - loss: 2.1393 - accuracy: 0.1896\n",
      "Epoch 00003: saving model to /Users/georgebrockman/code/georgebrockman/Autoenhance.ai/RoomDetection/cp.ckpt\n",
      "692/692 [==============================] - 560s 810ms/step - loss: 2.1393 - accuracy: 0.1896 - val_loss: 2.1340 - val_accuracy: 0.2087\n",
      "Epoch 4/10\n",
      "692/692 [==============================] - ETA: 0s - loss: 2.1397 - accuracy: 0.1919\n",
      "Epoch 00004: saving model to /Users/georgebrockman/code/georgebrockman/Autoenhance.ai/RoomDetection/cp.ckpt\n",
      "692/692 [==============================] - 507s 733ms/step - loss: 2.1397 - accuracy: 0.1919 - val_loss: 2.1343 - val_accuracy: 0.2087\n",
      "Epoch 5/10\n",
      "692/692 [==============================] - ETA: 0s - loss: 2.1400 - accuracy: 0.1938\n",
      "Epoch 00005: saving model to /Users/georgebrockman/code/georgebrockman/Autoenhance.ai/RoomDetection/cp.ckpt\n",
      "692/692 [==============================] - 514s 743ms/step - loss: 2.1400 - accuracy: 0.1938 - val_loss: 2.1349 - val_accuracy: 0.2087\n",
      "Epoch 6/10\n",
      "692/692 [==============================] - ETA: 0s - loss: 2.1401 - accuracy: 0.1912\n",
      "Epoch 00006: saving model to /Users/georgebrockman/code/georgebrockman/Autoenhance.ai/RoomDetection/cp.ckpt\n",
      "692/692 [==============================] - 669s 966ms/step - loss: 2.1401 - accuracy: 0.1912 - val_loss: 2.1343 - val_accuracy: 0.2087\n",
      "Epoch 7/10\n",
      "692/692 [==============================] - ETA: 0s - loss: 2.1384 - accuracy: 0.1943\n",
      "Epoch 00007: saving model to /Users/georgebrockman/code/georgebrockman/Autoenhance.ai/RoomDetection/cp.ckpt\n",
      "692/692 [==============================] - 569s 823ms/step - loss: 2.1384 - accuracy: 0.1943 - val_loss: 2.1348 - val_accuracy: 0.2087\n",
      "Epoch 8/10\n",
      "692/692 [==============================] - ETA: 0s - loss: 2.1390 - accuracy: 0.1938\n",
      "Epoch 00008: saving model to /Users/georgebrockman/code/georgebrockman/Autoenhance.ai/RoomDetection/cp.ckpt\n",
      "692/692 [==============================] - 513s 741ms/step - loss: 2.1390 - accuracy: 0.1938 - val_loss: 2.1343 - val_accuracy: 0.2087\n",
      "Epoch 9/10\n",
      "692/692 [==============================] - ETA: 0s - loss: 2.1382 - accuracy: 0.1970\n",
      "Epoch 00009: saving model to /Users/georgebrockman/code/georgebrockman/Autoenhance.ai/RoomDetection/cp.ckpt\n",
      "692/692 [==============================] - 495s 715ms/step - loss: 2.1382 - accuracy: 0.1970 - val_loss: 2.1339 - val_accuracy: 0.2087\n",
      "Epoch 10/10\n",
      "692/692 [==============================] - ETA: 0s - loss: 2.1390 - accuracy: 0.1964\n",
      "Epoch 00010: saving model to /Users/georgebrockman/code/georgebrockman/Autoenhance.ai/RoomDetection/cp.ckpt\n",
      "692/692 [==============================] - 536s 775ms/step - loss: 2.1390 - accuracy: 0.1964 - val_loss: 2.1353 - val_accuracy: 0.2087\n"
     ]
    }
   ],
   "source": [
    "initial_epochs = 10\n",
    "history = model.fit(train_dataset,\n",
    "                    epochs=initial_epochs,\n",
    "                    callbacks=[cp_callback],\n",
    "                    validation_data= test_dataset)\n",
    "model.save('room_detection.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfreeze the top layers of the model\n",
    "\n",
    "base_model.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers in the base model:  155\n"
     ]
    }
   ],
   "source": [
    "# show how many layers are in the basemodel\n",
    "print(\"Number of layers in the base model: \", len(base_model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tune for this number onwards\n",
    "fine_tune_at = 110\n",
    "\n",
    "# freeze all the layers before the tuning - this can be done with a for loop and slicing\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile \n",
    "\n",
    "base_learning_rate = 0.0001\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\n",
    "              # Only two linear outputs so use BinaryCrossentropy and logits =True\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "sequential (Sequential)      (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "tf_op_layer_RealDiv (TensorF [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Sub (TensorFlowO [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "mobilenetv2_1.00_224 (Functi (None, 7, 7, 1280)        2257984   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 9)                 11529     \n",
      "=================================================================\n",
      "Total params: 2,269,513\n",
      "Trainable params: 1,754,697\n",
      "Non-trainable params: 514,816\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20\n",
      "692/692 [==============================] - 611s 883ms/step - loss: 2.1408 - accuracy: 0.1917 - val_loss: 2.1397 - val_accuracy: 0.2087\n",
      "Epoch 11/20\n",
      "692/692 [==============================] - 613s 886ms/step - loss: 2.1403 - accuracy: 0.1907 - val_loss: 2.1371 - val_accuracy: 0.2087\n",
      "Epoch 12/20\n",
      "692/692 [==============================] - 673s 973ms/step - loss: 2.1405 - accuracy: 0.1937 - val_loss: 2.1348 - val_accuracy: 0.2087\n",
      "Epoch 13/20\n",
      "692/692 [==============================] - 618s 893ms/step - loss: 2.1392 - accuracy: 0.1921 - val_loss: 2.1343 - val_accuracy: 0.2087\n",
      "Epoch 14/20\n",
      "692/692 [==============================] - 616s 890ms/step - loss: 2.1391 - accuracy: 0.1919 - val_loss: 2.1343 - val_accuracy: 0.2087\n",
      "Epoch 15/20\n",
      "692/692 [==============================] - 616s 890ms/step - loss: 2.1401 - accuracy: 0.1962 - val_loss: 2.1339 - val_accuracy: 0.2087\n",
      "Epoch 16/20\n",
      "692/692 [==============================] - 607s 877ms/step - loss: 2.1411 - accuracy: 0.1908 - val_loss: 2.1347 - val_accuracy: 0.2087\n",
      "Epoch 17/20\n",
      "692/692 [==============================] - 614s 887ms/step - loss: 2.1410 - accuracy: 0.1915 - val_loss: 2.1339 - val_accuracy: 0.2087\n",
      "Epoch 18/20\n",
      "692/692 [==============================] - 611s 882ms/step - loss: 2.1396 - accuracy: 0.1946 - val_loss: 2.1344 - val_accuracy: 0.2087\n",
      "Epoch 19/20\n",
      "692/692 [==============================] - 609s 880ms/step - loss: 2.1395 - accuracy: 0.1940 - val_loss: 2.1342 - val_accuracy: 0.2087\n",
      "Epoch 20/20\n",
      "692/692 [==============================] - 603s 872ms/step - loss: 2.1395 - accuracy: 0.1938 - val_loss: 2.1342 - val_accuracy: 0.2087\n"
     ]
    }
   ],
   "source": [
    "fine_tune_epochs = 10\n",
    "total_epochs =  initial_epochs + fine_tune_epochs\n",
    "\n",
    "history_fine = model.fit(train_dataset,\n",
    "                         epochs=total_epochs,\n",
    "                         # initial epoch is the last one in the original training, use index -1\n",
    "                         initial_epoch=history.epoch[-1],\n",
    "                         validation_data=test_dataset)\n",
    "model.save('room_detection.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decrease learning rate\n",
    "# play around with different layers. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
